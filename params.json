{"name":"Asst3 pathtracer","tagline":"CMU 15-462/662 Assignment 3: A Mini-Path Tracer","body":"\r\n![Resampled mesh](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/teaser.jpg)\r\n\r\n### Due Date\r\nMonday, Nov 2nd, 11:59pm\r\n\r\n### Overview\r\n\r\nIn this project you will implement a simple path tracer that can render pictures with global illumination effects.  The first part of the assignment will focus on providing an efficient implementation of __ray-scene geometry queries__.  In the second half of the assignment you will __add the ability to simulate how light bounces around the scene__, which will allow your renderer to synthesize much higher-quality images.  Much like in assignment 2, input scenes are defined in COLLADA files, so you can create your own scenes for your scenes to render using free software like [Blender](http://blender.org).)\r\n\r\n### Getting started\r\n\r\nWe will be distributing assignments with git. You can find the repository for this assignment at <http://462cmu.github.io/asst3_pathtracer/>. If you are unfamiliar with git, here is what you need to do to get the starter code:\r\n```\r\n$ git clone https://github.com/462cmu/asst3_pathtracer.git\r\n```\r\nThis will create a <i class=\"icon-folder\"> </i> *asst3_pathtracer* folder with all the source files.\r\n\r\n### Build Instructions\r\n\r\nIn order to ease the process of running on different platforms, we will be using [CMake](http://www.cmake.org \"CMake Homepage\") for our assignments. You will need a CMake installation of version 2.8+ to build the code for this assignment. The GHC 5xxx cluster machines have all the packages required to build the project. It should also be relatively easy to build the assignment and work locally on your OSX or Linux. Building on Windows is currently not supported.\r\n\r\nIf you are working on OS X and do not have CMake installed, we recommend installing it through [Macports](https://www.macports.org/):\r\n```\r\nsudo port install cmake\r\n```\r\nOr [Homebrew](http://brew.sh/):\r\n```\r\nbrew install cmake\r\n```\r\n\r\nTo build your code for this assignment:\r\n\r\n- Create a directory to build your code:\r\n```\r\n$ cd asst3_pathtracer && mkdir build && cd build\r\n```\r\n- Run CMake to generate makefile:\r\n```\r\n$ cmake ..\r\n```\r\n- Build your code:\r\n```\r\n$ make\r\n```\r\n- (Optionally) install the executable (to asst3_pathtracer/bin):\r\n```\r\n$ make install\r\n```\r\n\r\n### Using the Path Tracer app\r\n\r\nWhen you have successfully built your code, you will get an executable named **pathtracer**. The **pathtracer** executable takes exactly one argument from the command line, which is the path of a COLLADA file describing the scene. For example, to load the Keenan cow <i class=\"icon-file\"> </i>*dae/meshEdit/cow.dae* from your build directory:\r\n\r\n```\r\n./pathtracer ../dae/meshEdit/cow.dae\r\n```\r\n\r\nThe following are `pathtracer` app command line options, which are provided for convenience and to debug debugging:\r\n\r\n| Commandline Option                              | Description                                                |\r\n| -------------------------- |:--------------------------------------------------|\r\n| `-t <INT>`               | Number of threads used for rendering (default=1) \r\n|\r\n| `-s <INT>`               | Set the number of camera rays per pixel (default=1) (should be a power of two)                                                                            |\r\n| `-l <INT>`               | Number of samples to integrate light from area light sources (default=1, higher numbers decrease noise but increase rendering time)\r\n| `-m <INT>`               | Maximum ray \"depth\" (the number of bounces on a ray path before the path is terminated)\r\n| `-h`               | Print command line help\r\n\r\n\r\n####Mesh Editor Mode\r\n\r\nWhen you first run the application, you will see an interactive wireframe view of the scene that should be familiar to you from Assignment 2. You can rotate the camera by left-clicking and dragging, zoom in/out using the scroll wheel (or multi-touch scrolling on a trackpad), and translate (dolly) the camera using right-click drag.  Hitting the spacebar will reset the view.  \r\n\r\nAs with assignment 2, you'll notice that mesh elements (faces, edges, and vertices) under the cursor are highlighted.  Clicking on these mesh elements will display information about the element and its associated data. The UI has all the same mesh editing controls as the MeshEdit app from Assignment 2 (listed below).  If you copy your implementation of these operators from Assignment 2 into your Assignment 3 codebase, then you will be able to edit scene geometry using the app. \r\n\r\n![PathTracer GUI](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/editor_ui.png)\r\n\r\n####Rendered Output Mode and BVH Visualization Mode\r\n\r\nIn addition to the mesh editing UI, the app features two other UI modes. Pressing the <kbd>R</kbd> key toggles display to the rendered output of your ray tracer.  If you press <kbd>R</kbd> in the starter code, you will see a black screen (You have not implemented your ray tracer yet! ).  However, a correct implementation of the assignment will make pictures of the cow that looks like the one below.  \r\n\r\n![PathTracer GUI](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/cow_render.png)\r\n\r\n Pressing <kbd>E</kbd> returns to the mesh editor view.  Pressing <kbd>V</kbd> displays the BVH visualizer mode, which will be a helpful visualization tool for debugging the bounding volume hierarchy you will need to implement for this assignment.  (More on this later.)\r\n\r\n### Summary of Viewer Controls\r\nA table of all the keyboard controls in the interactive mesh viewer part of the **pathtracer** application is provided below.\r\n\r\n| Command                              | Key                                                |\r\n| ------------------------------------ |:--------------------------------------------------|\r\n| Flip the selected edge               | <kbd>F</kbd>                                       |\r\n| Split the selected edge              | <kbd>S</kdb>                                       |\r\n| Collapse the selected edge           | <kbd>C</kdb>                                       |\r\n| Upsample the current mesh            | <kbd>U</kdb>                                       |\r\n| Downsample the current mesh          | <kbd>D</kdb>                                       |\r\n| Resample the current mesh            | <kbd>M</kdb>                                       |\r\n| Toggle information overlay           | <kbd>H</kdb>     \r\n|\r\n| Return to mesh edit mode             | <kbd>E</kdb>                                      \r\n| \r\n| Show BVH visualizer mode             | <kbd>V</kdb>\r\n| \r\n| Show ray traced output               | <kbd>R</kdb> \r\n| \r\n| Decrease area light samples (RT mode) | <kbd>-</kdb>\r\n| \r\n| Increase area light samples (RT mode) | <kbd>+</kdb>\r\n|\r\n| Decrease samples (camera rays) per pixel | <kbd>[</kdb>\r\n| \r\n| Increase samples (camera rays) per pixel | <kbd>]</kdb>              \r\n|\r\n| Descend to left child (BVH viz mode) | <kbd>LEFT</kdb>\r\n|\r\n| Descend to right child (BVH viz mode) | <kbd>RIGHT</kdb>\r\n|\r\n| Move to parent node (BVH viz mode) |  <kbd>UP</kdb>               \r\n|\r\n| Reset camera to default position     | <kbd>SPACE</kdb>                                   |                                      |\r\n| Edit a vertex position               | (left-click and drag on vertex)                         |\r\n| Rotate camera                        | (left-click and drag on background)     |\r\n|\r\n| Zoom camera                          | (mouse wheel)     |\r\n|\r\n| Dolly (translate) camera             | (right-click and drag on background) |\r\n\r\n### Getting Acquainted with the Starter Code\r\n\r\nFollowing the design of modern ray tracing systems, we have chosen to implement the ray tracing components of the Assignment 3 starter code in a very modular fashion.  Therefore, unlike previous assignments, your implementation will touch a number of files in the starter code.  The main structure of the code base is:\r\n\r\n* The main workhorse class is `Raytracer` defined in <i class=\"icon-file\"> </i> *raytracer.cpp*.  Inside the ray tracer class everything begins with the method `Raytracer::raytrace_pixel()` in <i class=\"icon-file\"> </i> *raytracer.cpp*.  This method computes the value of the specified pixel in the output image.\r\n* The camera is defined in the `Camera` class in <i class=\"icon-file\"> </i> *camera.cpp*.  You will need to modify `Camera::generate_ray()' in Part 1 of the assignment to generate the camera rays that are sent out into the scene.\r\n* Scene objects (e.g., triangles and spheres) are instances of the `Primitive` interface defined in <i class=\"icon-file\"> </i> *static_scene/Primitive.h*.  You will need to implement the `Primitive::intersect()` method for both triangles and spheres.\r\n* Lights implement the `Light` interface defined in  <i class=\"icon-file\"> </i> *static_scene/Light.h*.  The initial starter code has working implementations of directional lights and constant hemispherical lights.\r\n* Light energy is represented by instances of the `Spectrum` class.  While it's tempting, we encourage you to avoid thinking of spectrums as colors -- think of them as a measurement of energy over many wavelengths.  Although our current implementation only represents spectrums by red, green, and blue components (much like the RGB representations of color you've used previously in this class), this abstraction makes it possible to consider other implementations of spectrum in the future.  Spectrums can be converted into colors using the `Spectrum::toColor()` method. \r\n* A major portion of the first half of the assignment concerns implementing a bounding volume hierarchy (BVH) that accelerates ray-scene intersection queries.  The implementation of the BVH will be located in <i class=\"icon-file\"> </i> *bvh.cpp/.h*.  Note that a BVH is also an instance of the `Primitive` interface (A BVH is a scene primitive that itself contains other primitives.)\r\n\r\nPlease refer to the inline comments (or the Doxygen documentation) for further details.\r\n\r\n### Task 1: Generating Camera Rays\r\n\r\n\"Camera rays\" emanate from the camera and measure the amount of scene radiance that reaches a point on the camera's sensor plane.  (Given a point on the virtual sensor plane, there is a corresponding camera ray that is traced into the scene.)\r\n\r\nTake a look at `Raytracer::raytrace_pixel()` in <i class=\"icon-file\"> </i> *raytracer.cpp*.  The job of this function is to compute the amount of energy arriving at this pixel of the image. Conveniently, we've given you a function `Raytracer::trace_ray(r)` that provides a measurement of incoming scene radiance along the direction given by ray `r`.\r\n\r\nWhen the number of samples per pixel is 1, you should sample incoming radiance at the __center of each pixel__ by constructing a ray `r` that begins at this sensor location and travels through the camera's pinhole.   Once you have computed this ray, then call `Raytracer::trace_ray(r)` to get the energy deposited in the pixel.\r\n\r\n__Step 1:__ Given the width and height of the screen, and point in screen space, compute the corresponding coordinates of the point in normalized ([0-1]x[0x1]) screen space in `Raytracer::raytrace_pixel()`.  Pass these coordinates to the camera via `Camera::generate_ray()` in <i class=\"icon-file\"> </i> *camera.cpp*.  \r\n\r\n__Step 2:__ Implement `Camera::generate_ray()`. This function should return a ray __in world space__ that reaches the given sensor sample point.  We recommend that you compute this ray in camera space (where the camera pinhole is at the origin, the camera is looking down the -Z axis, and +Y is at the top of the screen.)  Note that the camera maintains camera-space-to-world space transform `c2w` that will be handy.\r\n\r\n__Step 3:__ Your implementation of `Raytracer::raytrace_pixel()` must support supersampling (more than one sample per pixel).  The member `Raytracer::ns_aa` in the raytracer class gives the number of samples of scene radiance your ray tracer should take per pixel (a.k.a. the number of camera rays per pixel.  Note that `Raytracer::sampler2d->get_sample()`  provides uniformly distributed random 2D points in the [0-1]^2 box (see the implementation in <i class=\"icon-file\"> </i> *sampler.cpp*).\r\n\r\n__Tips:__ Since it'll be hard to know if you camera rays are correct until you implement primitive intersection, we recommend debugging your camera rays by checking what your implementation of `Camera::generate_ray()` does with rays at the center of the screen (0.5, 0.5) and at the corners of the image.\r\n\r\n__Extra credit ideas:__\r\n\r\n* Modify the implementation of the camera to simulate a camera with a finite aperture (rather than a pinhole camera).  This will allow your ray tracer to simulate the effect of defocus blur.\r\n* Write your own `Sampler2D` implementation that generates samples with improved distribution.  Some examples include:\r\n * Jittered Sampling\r\n * Multi-jittered sampling\r\n * N-Rooks (Latin Hypercube) sampling\r\n * Sobol sequence sampling\r\n * Halton sequence sampling\r\n * Hammersley sequence sampling\r\n\r\n### Task 2: Intersecting Triangles and Spheres\r\n\r\nNow that your ray tracer generates camera rays, you need to implement ray-primitive intersection routines for the two primitives in the starter code: triangles and spheres.  This handout will discuss the requirements of intersecting primitives in terms of triangles.\r\n\r\nThe `Primitive` interface contains two types of intersection routines:\r\n\r\n* `bool Triangle::intersect(const Ray& r)` returns true/false depending on whether ray `r` hits the triangle.\r\n\r\n* `bool Triangle::intersect(const Ray& r, Intersection *isect)` returns true/false depending on whether ray `r` hits the triangle, but also populates an `Intersection` structure with information describing the surface at the point of the hit.\r\n\r\nYou will need to implement both of these routines. Correctly doing so requires you to understand the fields in the `Ray` structure defined in <i class=\"icon-file\"> </i> *ray.h*.\r\n\r\n* `Ray.o` represents the 3D point of origin of the ray\r\n* `Ray.d` represents the 3D direction of the ray (this direction will be normalized)\r\n* `Ray.min_t` and `Ray.max_t` correspond to the minimum and maximum points on the ray.  That is, intersections that lie outside the  `Ray.min_t` and `Ray.max_t` range __should not__ be considered valid intersections with the primitive.\r\n\r\nThere are also two additional fields in the `Ray` structure that can be helpful in accelerating your intersection computations with bounding boxes (see the `BBox` class in <i class=\"icon-file\"> </i> *bbox.h*).  You may or may not find these precomputed values helpful in your computations.\r\n\r\n* `Ray.inv_d` is a vector holding (1/x, 1/d.y, 1/d.z)\r\n* `Ray.sin[3]` hold indicators of the sign of each component of the ray's direction.\r\n\r\nOne important detail of the `Ray` structure is that `min_t` and `max_t` are `mutable` fields of the `Ray`.  This means that these fields can be modified by constant member functions such as `Triangle::Intersect()`.  When finding the first intersection of a ray and the scene, you almost certainly want to update the ray's `max_t` value after finding hits with scene geometry.  By bounding the ray as tightly as possible, your ray tracer will be able to avoid unnecessary tests with scene geometry that is known to not be able to result in a closest hit, resulting in higher performance.\r\n\r\n__Step 1: Intersecting Triangles__\r\n\r\nWhile faster implementations are possible, we recommend you implement ray-triangle intersection using the method described in the [lecture slides](http://15462.courses.cs.cmu.edu/fall2015/lecture/acceleration/slide_004). Further details of implementing this method efficiently are given in [these notes](http://15462.courses.cs.cmu.edu/fall2015/article/15).\r\n\r\nThere are two important details you should be aware of about intersection:\r\n\r\n* When finding the first-hit intersection with a triangle, you need to fill in the `Intersection` structure with details of the hit.  The structure should be initialized with:\r\n  * `t`: the ray's t-value of the hit point\r\n  * `n`: the normal of the surface at the hit point.  This normal should be the interpolated normal (obtained via interpolation of the per-vertex normals according to the barycentric coordinates of the hit point)\r\n  * `primitive`: a pointer to the primitive  that was hit\r\n  * `bsdf`: a pointer to the surface brdf at the hit point (obtained via `mesh->get_bsdf()`) \r\n\r\n* When intersection occurs with the back-face of a triangle (the side of the triangle opposite the direction of the normal) you should __return the normal of triangle pointing away from the side of the triangle that was hit.__\r\n\r\nOnce you've successfully implemented triangle intersection, you will be able to render many of the scenes in the scenes directory (<i class=\"icon-folder\"> </i> */dae*)).  However, your ray tracer will be __very slow!__\r\n\r\n__Step 2: Intersecting Spheres__\r\n\r\nPlease also implement the intersection routines for the `Sphere` class in  <i class=\"icon-file\"> </i> *sphere.cpp*.  Remember that your intersection tests should respect the ray's `min_t` and `max_t` values.\r\n\r\n### Task 3: Implementing a Bounding Volume Hierarchy (BVH)\r\n\r\nIn this task you will implement a bounding volume hierarchy that accelerates ray-scene intersection.  All of this work will be in the `BVHAccel` class in <i class=\"icon-file\"> </i> *bvh.cpp*.  \r\n\r\nThe starter code constructs a valid BVH, but it is a trivial BVH with a single node containing all scene primitives.  A `BVHNode` has the following fields:\r\n\r\n* `BBox bb`: the bounding box of the node  (bounds all primitives in the subtree rooted by this node)                                                                                   \r\n* `int start`:  start index of primitives in the BVH's primitive array                                                                          \r\n* `size_t range`:   range of index in the primitive list (number of primitives in the subtree rooted by the node)                                                                       \r\n* `BVHNode* l`:     left child node                                                                                              \r\n* `BVHNode* r`:     right child node    \r\n\r\nThe `BVHAccel` class maintains an array of all primitives in the BVH (`primitives`).  The fields `start` and `range` in the `BVHNode` refer the range of contained primitives in this array.\r\n\r\n__Step 1:__ Your job is to construct a BVH using the [Surface Area Heuristic](http://15462.courses.cs.cmu.edu/fall2015/lecture/acceleration/slide_024) discussed in class.  Tree construction should occur when the `BVHAccel` object is constructed.\r\n\r\nWe have implemented a number of tools to help you debug the BVH.  Press the <kbd>V</kbd> key to enter BVH visualization mode.  This mode allows you to directly visualize a BVH as shown below.  The current BVH node is highlighted in red.  Primitives in the left and right subtrees of the current BVH node are rendered in different colors.  __Press the <kbd>LEFT</kbd> or <kbd>RIGHT</kbd> keys to descend to child nodes of the mesh. Press <kbd>UP</kbd>__ to move the parent of the current node.\r\n\r\n![BVH Vis](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/cow_bvh.png)\r\n\r\nAnother view showing the contents of a lower node in the BVH:\r\n\r\n![BVH Vis](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/cow_bvh_2.png)\r\n\r\n__Step 2:__ Implement the ray-BVH intersection routines required by the `Primitive` interface.  You may wish to consider the node visit order optimizations we discussed in class.  Once complete, your renderer should be able to render all of the test scenes in a reasonable amount of time.\r\n\r\n### Task 4: Implementing Shadow Rays\r\n\r\nIn this task you will modify `Raytracer::trace_ray()` to implement accurate shadows.\r\n\r\n Currently `trace_ray` computes the following:\r\n\r\n* It computes the intersection of ray `r` with the scene.\r\n* It computes the amount of light arriving at the hit point `hit_p` (the irradiance at the hit point) by integrating radiance from all scene light sources.\r\n* It computes the radiance reflected from the `hit_p` in the direction of `-r`. (The amount of reflected light is based on the brdf of the surface at the hit point.)\r\n\r\nShadows occur when another scene object blocks light emitted from scene light sources towards the hit point (`hit_p`).  Fortunately, determining whether or not a ray of light from a light source to the hit point is occluded by another object is easy given a working ray tracer (which you have at this point!).  __You simply want to know whether a ray originating from the hit point (`p_hit`), and traveling towards the light source (`dir_to_light`) hits any scene geometry before reaching the light (note, the light's distance from the hit point is given by `dist_to_light`).__   \r\n\r\nYour job is to implement the logic needed to compute whether `hit_p` is in shadow with respect to the current light source sample.  Below are a few tips:\r\n\r\n* A common ray tracing pitfall is for the \"shadow ray\" shot into the scene to accidentally hit the same triangle as `r` (the surface is erroneously determined to be occluded because the shadow ray is determined to hit the surface!).  We recommend that you make sure the origin of the shadow ray is offset from the surface to avoid these erroneous \"self-intersections\".  For example, `o` = `p_hit + epsilon * dir_to_light`  (note: `EPS_D` is defined for this purpose).\r\n* You will find it useful to debug your shadow code using the `DirectionalLight` since it produces hard shadows that are easy to reason about.  \r\n\r\nAt this point you should be able to render very striking images. For example, here is the Stanford Dragon model rendered with both a directional light and a hemispherical light.\r\n \r\n ![Shadow directional](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/shadow_directional.png)\r\n\r\n![Shadow directional](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/shadow_hemisphere.png)\r\n \r\n### Task 5: Adding Path Tracing\r\n\r\n__A few notes before getting started:__\r\n\r\nThe new release of the starter code for tasks 5-7 makes a few changes and improvements to the original starter code of the assignment:\r\n\r\n* The `BRDF` class has been renamed `BSDF` (for \"bidirectional scattering distribution function\") to indicate that the class is now responsible for computing both light that is reflected from the surface, but also light that is refracted as it is transmitted through the surface\r\n \r\n* Rather than use a single hardcoded light source in your `Pathtracer::trace_ray()` lights are now defined as part of the scene description file.  \r\n\r\nYou should change your implementation of the reflectance estimate due to direct lighting in `Pathtracer::trace_ray()`  to iterate over the list of scene light sources using the following code:\r\n\r\n    for (SceneLight* light : scene->lights) {\r\n        /// do work here...\r\n    }\r\n\r\n------\r\n\r\nIn this task you will modify your ray tracer to add support for indirect illumination.  We wish for you to implement the path tracing algorithm that terminates ray paths using Russian Roulette, as discussed in class.  Recommend that you restructure the code in `Pathtracer::trace_ray` as follows:\r\n\r\n    Pathtracer::trace_ray() {\r\n      if (surface hit) {\r\n           //\r\n           // compute reflectance due to direct lighting only         \r\n           //\r\n           for each light:\r\n              accumulate reflectance contribution due to light\r\n             \r\n           //   \r\n           // add reflectance due to indirect illumination\r\n           //\r\n           randomly select a new ray direction (it may be    \r\n           reflection or transmittence ray depending on \r\n           surface type -- see BSDF::sample_f()\r\n\r\n           potentially kill path (using Russian roulette)\r\n\r\n           evaluate weighted reflectance contribution due\r\n           to light from this direction\r\n      }\r\n    }\r\n\r\nAfter correctly implementing path tracing your renderer should be able to make a beautifully lit picture like this (note: the image of the simple Cornell Box below was rendering using 256 camera rays per pixel, and thus takes a lot of time to render!):\r\n\r\n    ./pathtracer -l 1 -s 256 -m 4 -t 7 ../dae/sky/CBspheres_lambertian.dae\r\n\r\n ![Cornell box 256](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/cb_lambertian_256.png)\r\n\r\n ![Cornell box 256 close](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/cb_lambertian_close.png)\r\n\r\nHere is a quicker (but much noisier) rendering using only 1 path per pixel:\r\n\r\n./pathtracer -l 1 -s 1 -m 4 -t 7 ../dae/sky/CBspheres_lambertian.dae\r\n\r\n ![Cornell box 1](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/cb_lambertian_1.png)\r\n\r\nHere are a few tips:\r\n\r\n* The termination probability of paths can be determined based on the [overall throughput](http://15462.courses.cs.cmu.edu/fall2015/lecture/globalillum/slide_044) of the path (you'll likely need to add a field to the `Ray` structure to implement this) or based on the value of the BSDF given `wo` and `wi` in the current step.  Keep in mind that delta function BRDFs can take on values greater than one, so clamping termination probabilities derived from BRDF values to 1 is wise.\r\n\r\n* To convert a `Spectrum` to a termination probability, we recommend you use the luminance (overall brightness) of the Spectrum, which is available via `Spectrum::illum()`\r\n\r\n* We've given you some [pretty good notes](http://15462.courses.cs.cmu.edu/fall2015/lecture/globalillum/slide_047) on how to do this part of the assignment, but it can still be tricky to get correct.\r\n\r\n### Task 6: Adding New Materials \r\n\r\nNow that you have implemented the ability to sample more complex light paths, it's finally time to add support for more types of materials (other than the fully Lambertian material provided to you in the starter code).  In this task you will add support for two types of materials: a perfect mirror and glass (a material featuring both specular reflection and transmittance).\r\n\r\nTo get started take a look at the `BSDF` interface in <i class=\"icon-file\"> </i> *bsdf.cpp*.  There are a number of key methods you should understand:\r\n\r\n* `BSDF::f(wo,wi)` evaluates the distribution function for a given pair of directions.\r\n* `BSDF::sample_f(const Vector3D& wo, Vector3D* wi, float* pdf)` generates a random sample `wo` (which may be a reflection direction or a refracted transmitted light direction). The method returns the value of the distribution function for the pair of directions, and the pdf for the selected sample `wi`.\r\n\r\nThere are also two helper functions in the BSDF class that you will need to implement:\r\n\r\n* `BSDF::reflect(w0, ...)` returns a direction `wi` that is the __perfect specular reflection direction__ corresponding to `wi` (reflection of w0 about the normal, which in the surface coordinate space is [0,0,1]).  More detail about specular reflection [is here](http://15462.courses.cs.cmu.edu/fall2015/lecture/reflection/slide_028).\r\n\r\n*  `BSDF::refract(w0, ...)` returns the ray that results from refracting the ray `w0`  about the surface according to [Snell's Law](http://15462.courses.cs.cmu.edu/fall2015/lecture/reflection/slide_032).  The surface's index of refraction is given by the argument `ior`.  Your implementation should assume that if the ray `w0` is __entering the surface__ (that is, if `cos(w0,N) > 0`) then the ray is currently in vacuum (index of refraction = 1.0).  If `cos(w0,N) < 0` then your code should assume the ray is leaving the surface and entering vacuum. __In the case of total internal reflection, the method should return `false`.__\r\n\r\n__What you need to do:__\r\n\r\n1. Implement the class `MirrorBSDF` which represents a  material with perfect specular reflection (a perfect mirror).  You should Implement `MirrorBSDF::f()`, `MirrorBSFD::sample_f()`, and `BSDF::reflect()`.  __(Hint: what should the pdf computed by `MirrorBSFD::sample_f()` be?  What should the reflectance function f() be?)__\r\n\r\n2. Implement the class  `GlassBSDF` which is a glass-like material that both reflects light and transmit light.  As discussed [in class](http://15462.courses.cs.cmu.edu/fall2015/lecture/reflection/slide_035) the fraction of light that is reflected and transmitted through glass is given by the __dielectric Fresnel equations__, which are [documented in detail here](http://15462.courses.cs.cmu.edu/fall2015/article/17/edit).  Specifically your implementation should:\r\n * `Implement BSDF::refract()` to add support for refracted ray paths.\r\n * Use the Fresnel equations to compute the fraction of reflected light and the fraction of transmitted light. Your implementation of\r\n *  Implement `GlassBSDF::sample_f()`.  Your implementation should use the Fresnel equations to compute the fraction of reflected light and the fraction of transmitted light.  The returned ray sample should be either a reflection ray or a refracted ray, with the probability of which type of ray to use for the current path proportional to the Fresnel reflectance. (e.g., If the Fresnel reflectance is 0.9, then you should generate a reflection ray 90% of the time. __What should the pdf be in this case?__)\r\n * You may wish to read [Kayvon's notes](http://15462.courses.cs.cmu.edu/fall2015/article/17/edit) on Fresnel equations as well as on how to compute a transmittance BRDF. \r\n \r\nWhen you are done, you will be able to render images with as these:\r\n\r\n ![Cornell box spheres 256](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/cb_spheres_256.png)\r\n\r\n### Task 7: Infinite Environment Lighting \r\n\r\nThe final task of this assignment will be to implement a new type of light source: an infinite environment light.  An environment light is a light that supplies incident radiance (really, the light intensity $\\frac{d\\Phi}{d\\omega}$) from all directions on the sphere.  The source is thought to be \"infinitely far away\", and is representative of realistic lighting environments in the real world: as a result, rendering using environment lighting can be quite striking.\r\n\r\nThe intensity of incoming light from each direction is defined by a texture map parameterized by $\\phi$ and $\\theta$, as shown below.\r\n\r\n ![Environment map](http://15462.courses.cs.cmu.edu/fall2015content/misc/asst3_images/envmap_figure.jpg)\r\n\r\nIn this task you need to implement the `EnvironmentLight::sample_L()` method in <i class=\"icon-file\"> </i> *static_scene/environment_light.cpp*.  You'll start with uniform direction sampling to get things working, and then move to a more advanced implementation that uses __importance sampling__ to significantly reduce variance in rendered images.\r\n\r\n__Step one: uniform sampling__\r\n\r\nTo get things working, your first implementation of `EnvironmentLight::sample_L()` will be quite simple.  You should generate a random direction on the sphere (__with uniform $\\frac{1}{4\\pi}$ probability with respect to solid angle__), convert this direction to coordinates $(\\phi,\\theta)$ and then look up the appropriate radiance value in the texture map using __bilinear interpolation__ (note: we recommend you begin with bilinear interpolation to keep things simple.)\r\n\r\n__Tips:__\r\n\r\n* You must write your own code to uniformly sample the sphere.\r\n* `envMap->data` contains the pixels of the environment map\r\n* The size of the environment texture is given by `envMap->w` and `envMap->h`.\r\n\r\n__Step two: importance sampling the environment map__\r\n\r\nMuch like light in the real world, most of the energy provided by an environment light source is concentrated in the directions toward bright light sources. __Therefore, it makes sense to bias selection of sampled directions towards the directions for which incoming radiance is the greatest.__  In this final task you will implement an importance sampling scheme for environment lights.  For environment lights with large variation in incoming light intensities, good importance sampling will significantly improve the quality of renderings.\r\n\r\nThe basic idea is that you will assign a probability to each pixel in the environment map based on the total flux passing through the solid angle it represents.  Specifically, a pixel with coordinate $\\theta=\\theta_0$ subtends an area $\\sin \\theta \\, d\\theta \\, d\\phi$ on the unit sphere (where $d\\theta$ and $d\\phi$ the angles subtended by each pixel -- as determined by the resolution of the texture).  Thus, the flux through a pixel is proportional to $L \\, \\sin \\theta$.   (We only care about the relative flux through each pixel to create a distribution.)\r\n\r\n__Summing the fluxes for all pixels, then normalizing the values so that they sum to one, yields a discrete probability distribution for picking a pixel based on flux through its corresponding solid angle on the sphere.__\r\n\r\nThe question is now how to sample from this 2D discrete probability distribution.  We recommend the following process which reduces the problem to drawing samples from two 1D distributions, each time using the inversion method discussed in class:\r\n\r\n* Given p($\\phi$, $\\theta$) the probability distribution for all pixels, compute the marginal probability distribution p($\\theta$) = $\\sum_{\\phi}$ p($\\phi$, $\\theta$) for selecting a value from each row of pixels.\r\n* Given for any pixel, compute the conditional probability p($\\phi$ | $\\theta$) as $\\frac{p(\\phi, \\theta)}{p(\\theta)}$.\r\n\r\nGiven the marginal distribution for $\\theta$ and the conditional distributions p($\\phi$ | $\\theta$) for environment map rows, it is easy to select a pixel as follows:\r\n\r\n1. Use the inversion method to first select a \"row\" of the environment map according to p($\\theta$).\r\n2. Given this row, use the inversion method to select a pixel in the row according to p($\\phi$ | $\\theta$).\r\n\r\n__Here are a few tips:__\r\n\r\n* When computing areas corresponding to a pixel, use the value of $\\theta$ at the pixels center.\r\n*  We recommend precomputing the joint distributions p($\\phi$, $\\theta$) and marginal distributions p($\\theta$) in the constructor of `EnvironmentLight` and storing the resulting values in fields of the class.\r\n* `Spectrum::illum()` returns the luminance (brightness) of a Spectrum.  The probability of a pixel should be proportional to the product of its luminance and the solid angle it subtends.\r\n* `std::binary_search` is your friend. Documentation [is here](http://en.cppreference.com/w/cpp/algorithm/binary_search).\r\n\r\n### Grading\r\n\r\nYour code must run on the GHC 5xxxx cluster machines as we will grade on those machines. Do not wait until the submission deadline to test your code on the cluster machines. Keep in mind that there is no perfect way to run on arbitrary platforms. If you experience trouble building on your computer, while the staff may be able to help, but the GHC 5xxx machines will always work and we recommend you work on them.\r\n\r\nThe assignment consists of a total of 100 pts. The point breakdown is as follows:\r\n\r\n* Task 1:  10\r\n* Task 2:  15\r\n* Task 3:  20\r\n* Task 4:  10\r\n* Task 5:  15\r\n* Task 6:  15\r\n* Task 7:  15\r\n\r\n### Handin Instructions\r\n\r\nYour handin directory is on AFS under:\r\n```\r\n/afs/cs/academic/class/15462-f15-users/ANDREWID/asst3/\r\n```\r\nYou will need to create the `asst3` directory yourself. All your files should be placed there. Please make sure you have a directory and are able to write to it well before the deadline; we are not responsible if you wait until 10 minutes before the deadline and run into trouble. Also, you may need to run `aklog cs.cmu.edu` after you login in order to read from/write to your submission directory.\r\n\r\nYou should submit all files needed to build your project, this include:\r\n\r\n* The `src` folder with all your source files\r\n\r\nPlease do not include:\r\n\r\n* The`build` folder\r\n* Executables\r\n* Any additional binary or intermediate files generated in the build process.\r\n\r\nYou should include a `README` file (plaintext or pdf) if you have implemented any of the extra credit features. In your `README` file, clearly indicate which extra credit features you have implemented. You should also briefly state anything that you think the grader should be aware of.\r\n\r\nDo not add levels of indirection when submitting. And please use the same arrangement as the handout. We will enter your handin directory, and run:\r\n```\r\nmkdir build && cd build && cmake .. && make\r\n```\r\nand your code should build correctly. The code must compile and run on the GHC 5xxx cluster machines. Be sure to check to make sure you submit all files and that your code builds correctly.\r\n\r\n### Friendly Advice from your TAs\r\n\r\n* As always, start early.  There is a lot to implement in this assignment, and no official checkpoint, so don't fall behind!\r\n\r\n* While C has many pitfalls, C++ introduces even more wonderful ways to shoot yourself in the foot. It is generally wise to stay away from as many features as possible, and make sure you fully understand the features you do use.\r\n\r\n### Resources and Notes\r\n\r\n* [Bryce's C++ Programming Guide](https://github.com/Bryce-Summers/Writings/blob/master/Programming%20Guides/C_plus_plus_guide.pdf)\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}